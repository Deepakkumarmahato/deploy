# -*- coding: utf-8 -*-
"""Untitled59.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13-po2iOKl_gWAa2YTEpRxDW9ruz0m4LE
"""



!pip install transformers==4.37.2 datasets huggingface_hub accelerate

import torch
import torch.nn as nn
from transformers import (
    AutoModel,
    DebertaV2TokenizerFast,
    get_linear_schedule_with_warmup,
    AutoModelForSequenceClassification,
)
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
import pandas as pd
import numpy as np
from tqdm import tqdm

# ---- CONFIG ----
MODEL_NAME = "microsoft/deberta-v3-base"
TARGETS = ["anger", "fear", "joy", "sadness", "surprise"]
MAX_LEN = 256
BATCH_SIZE = 16
EPOCHS = 5
LR = 2e-5

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DEVICE

train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

print(train.head())
print(train[TARGETS].sum())

class EmotionDataset(Dataset):
    def __init__(self, df, tokenizer, has_labels=True):
        self.texts = df["text"].tolist()
        self.labels = df[TARGETS].values if has_labels else None
        self.tokenizer = tokenizer
        self.has_labels = has_labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        enc = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=MAX_LEN,
            return_tensors="pt",
        )
        item = {
            "input_ids": enc["input_ids"].squeeze(0),
            "attention_mask": enc["attention_mask"].squeeze(0),
        }
        if self.has_labels:
            item["labels"] = torch.tensor(self.labels[idx], dtype=torch.float)
        return item

tokenizer = DebertaV2TokenizerFast.from_pretrained(MODEL_NAME)

train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)

train_ds = EmotionDataset(train_df, tokenizer)
val_ds = EmotionDataset(val_df, tokenizer)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)

len(train_loader), len(val_loader)

class DebertaMultiLabel(nn.Module):
    def __init__(self, model_name, num_labels):
        super().__init__()
        self.base = AutoModel.from_pretrained(model_name)
        hidden = self.base.config.hidden_size
        self.dropout = nn.Dropout(0.2)
        self.classifier = nn.Linear(hidden, num_labels)
        self.loss_fn = nn.BCEWithLogitsLoss()

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)
        pooled = outputs.last_hidden_state[:, 0, :]  # [CLS] token
        pooled = self.dropout(pooled)
        logits = self.classifier(pooled)

        if labels is not None:
            loss = self.loss_fn(logits, labels)
            return loss, logits
        return logits

model = DebertaMultiLabel(MODEL_NAME, num_labels=len(TARGETS)).to(DEVICE)

optimizer = torch.optim.AdamW(model.parameters(), lr=LR)
num_steps = len(train_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(optimizer, 0, num_steps)

model

def train_one_epoch():
    model.train()
    total = 0.0
    for batch in tqdm(train_loader, desc="Training"):
        optimizer.zero_grad()
        ids = batch["input_ids"].to(DEVICE)
        mask = batch["attention_mask"].to(DEVICE)
        labels = batch["labels"].to(DEVICE)

        loss, _ = model(ids, mask, labels)
        loss.backward()
        optimizer.step()
        scheduler.step()

        total += loss.item()
    return total / len(train_loader)


def validate():
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validating"):
            ids = batch["input_ids"].to(DEVICE)
            mask = batch["attention_mask"].to(DEVICE)
            labels = batch["labels"].cpu().numpy()

            logits = model(ids, mask)
            probs = torch.sigmoid(logits).cpu().numpy()
            preds.extend((probs > 0.5).astype(int))
            trues.extend(labels)
    return f1_score(trues, preds, average="macro")

for epoch in range(EPOCHS):
    print(f"\nEpoch {epoch+1}/{EPOCHS}")
    train_loss = train_one_epoch()
    val_f1 = validate()
    print(f"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val F1: {val_f1:.4f}")

from transformers import AutoModelForSequenceClassification

EXPORT_DIR = "/content/export_model"


torch.save(model.state_dict(), "/content/full_trained_model.pth")
print("✔ Saved full trained model weights to /content/full_trained_model.pth")


hf_model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=len(TARGETS),
)


state_dict = torch.load("/content/full_trained_model.pth", map_location="cpu")

new_dict = {}
for key in state_dict:
    if key.startswith("base."):
        new_key = key.replace("base.", "")
    elif key.startswith("classifier."):
        new_key = key
    else:
        new_key = key
    new_dict[new_key] = state_dict[key]

missing, unexpected = hf_model.load_state_dict(new_dict, strict=False)
print("Missing keys:", missing)
print("Unexpected keys:", unexpected)

hf_model.eval()

hf_model.config.problem_type = "multi_label_classification"
hf_model.config.id2label = {i: label for i, label in enumerate(TARGETS)}
hf_model.config.label2id = {label: i for i, label in enumerate(TARGETS)}


hf_model.save_pretrained(EXPORT_DIR)
tokenizer.save_pretrained(EXPORT_DIR)

print("✔ Export successful — files available at", EXPORT_DIR)

test_tok = DebertaV2TokenizerFast.from_pretrained(EXPORT_DIR)
test_model = AutoModelForSequenceClassification.from_pretrained(EXPORT_DIR)

sample_texts = [
    "I am very happy today!",
    "I am really angry and frustrated.",
    "I feel sad and alone.",
]

for txt in sample_texts:
    inputs = test_tok(txt, return_tensors="pt")
    with torch.no_grad():
        logits = test_model(**inputs).logits
        probs = torch.sigmoid(logits).numpy()[0]
    print("\nTEXT:", txt)
    print({label: float(p) for label, p in zip(TARGETS, probs)})

from huggingface_hub import HfApi, notebook_login

notebook_login()  # opens a prompt, paste your HF token

HF_MODEL_ID = "Deepak8409/deberta-emotion-classifier"  # your repo

api = HfApi()
api.upload_folder(
    folder_path=EXPORT_DIR,
    repo_id=HF_MODEL_ID,
    repo_type="model",
    commit_message="Uploading fine-tuned DeBERTa emotion model",
)
print("✔ Uploaded successfully to:", HF_MODEL_ID)